{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# import torch\n",
    "\n",
    "# MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=None\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0\n",
    "# )\n",
    "\n",
    "# def generate_insight(prompt: str) -> str:\n",
    "#     response = generator(\n",
    "#         prompt,\n",
    "#         do_sample=True,\n",
    "#         max_new_tokens=200,\n",
    "#         temperature=0.7,\n",
    "#         top_k=50,\n",
    "#         top_p=0.95,\n",
    "#         eos_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "#     return response[0][\"generated_text\"];\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_prompt = \"You are a data scientist. Analyze this metadata: age, region, income. What trends can you see? Give short answers. Be precise and analytical.\"\n",
    "#     print(generate_insight(test_prompt))\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "def generate_insight(prompt: str, max_tokens: int = 512) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Prevents warnings\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated[len(prompt):].strip()  # remove original prompt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = (\n",
    "        \"You are a data scientist. Analyze this metadata: age, region, income. Give 3 short points and stop generating\"\n",
    "    )\n",
    "    print(generate_insight(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532129a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2910f94",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8eae049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saksh\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5344bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, max_tokens: int = 200) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded[len(chat_prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae66b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"agentic_metadata\\metadata.csv\").head(5)\n",
    "df.dropna(axis = 1, inplace = True)\n",
    "df = df.round(2)\n",
    "# df = df.to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8ef8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Measure Type</th>\n",
       "      <th>Measure</th>\n",
       "      <th>Group</th>\n",
       "      <th>Subgroup</th>\n",
       "      <th>Estimate Type</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Lower 95% CI</th>\n",
       "      <th>Upper 95% CI</th>\n",
       "      <th>Reliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan.-Jun. 2022</td>\n",
       "      <td>Diagnosis Chapter</td>\n",
       "      <td>All visits</td>\n",
       "      <td>Total</td>\n",
       "      <td>All visits</td>\n",
       "      <td>Visit count</td>\n",
       "      <td>54013000.0</td>\n",
       "      <td>9564000.0</td>\n",
       "      <td>35267000.0</td>\n",
       "      <td>72759000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan.-Jun. 2022</td>\n",
       "      <td>Diagnosis Chapter</td>\n",
       "      <td>All visits</td>\n",
       "      <td>Total</td>\n",
       "      <td>All visits</td>\n",
       "      <td>Visit rate (per 1,000 people)</td>\n",
       "      <td>165.1</td>\n",
       "      <td>29.2</td>\n",
       "      <td>107.8</td>\n",
       "      <td>222.4</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan.-Jun. 2022</td>\n",
       "      <td>Diagnosis Chapter</td>\n",
       "      <td>All visits</td>\n",
       "      <td>By age</td>\n",
       "      <td>0-17 years old</td>\n",
       "      <td>Visit count</td>\n",
       "      <td>9568000.0</td>\n",
       "      <td>1869000.0</td>\n",
       "      <td>5905000.0</td>\n",
       "      <td>13231000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan.-Jun. 2022</td>\n",
       "      <td>Diagnosis Chapter</td>\n",
       "      <td>All visits</td>\n",
       "      <td>By age</td>\n",
       "      <td>0-17 years old</td>\n",
       "      <td>Visit rate (per 1,000 people)</td>\n",
       "      <td>130.3</td>\n",
       "      <td>25.5</td>\n",
       "      <td>80.4</td>\n",
       "      <td>180.2</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan.-Jun. 2022</td>\n",
       "      <td>Diagnosis Chapter</td>\n",
       "      <td>All visits</td>\n",
       "      <td>By age</td>\n",
       "      <td>18-44 years old</td>\n",
       "      <td>Visit count</td>\n",
       "      <td>19253000.0</td>\n",
       "      <td>3346000.0</td>\n",
       "      <td>12696000.0</td>\n",
       "      <td>25811000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time Period       Measure Type     Measure   Group         Subgroup  \\\n",
       "0  Jan.-Jun. 2022  Diagnosis Chapter  All visits   Total       All visits   \n",
       "1  Jan.-Jun. 2022  Diagnosis Chapter  All visits   Total       All visits   \n",
       "2  Jan.-Jun. 2022  Diagnosis Chapter  All visits  By age   0-17 years old   \n",
       "3  Jan.-Jun. 2022  Diagnosis Chapter  All visits  By age   0-17 years old   \n",
       "4  Jan.-Jun. 2022  Diagnosis Chapter  All visits  By age  18-44 years old   \n",
       "\n",
       "                   Estimate Type    Estimate  Standard Error  Lower 95% CI  \\\n",
       "0                    Visit count  54013000.0       9564000.0    35267000.0   \n",
       "1  Visit rate (per 1,000 people)       165.1            29.2         107.8   \n",
       "2                    Visit count   9568000.0       1869000.0     5905000.0   \n",
       "3  Visit rate (per 1,000 people)       130.3            25.5          80.4   \n",
       "4                    Visit count  19253000.0       3346000.0    12696000.0   \n",
       "\n",
       "   Upper 95% CI Reliable  \n",
       "0    72759000.0      Yes  \n",
       "1         222.4      Yes  \n",
       "2    13231000.0      Yes  \n",
       "3         180.2      Yes  \n",
       "4    25811000.0      Yes  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8eb1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define messages\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a professional data scientist with deep expertise in analyzing tabular data, detecting statistical trends, and drawing concise, data-driven conclusions.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Analyze this metadata: age, region, income. Give 3 precise, short analytical points. Stop generating after that.\"}\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert epidemiologist. Analyze the following structured health data. \"\n",
    "            \"Give exactly 3 short, unique insights based on real values. Stop once complete. \"\n",
    "            \"Avoid repeating numbers, age groups or anything, only focus on things that are unique across the rows\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is the metadata:\\n\\n{df}\\n\\nGive 3 insights using epidemiological reasoning.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a professional data scientist. You analyze structured tabular data to extract insights using statistical reasoning. \"\n",
    "#         \"Base your answers strictly on the data provided and do not assume missing values.\"\n",
    "#         \"Give exactly 3 short, unique insights based on real values. Stop once complete. \"\n",
    "#         \"Avoid repeating numbers, age groups or anything, only focus on things that are unique across the rows\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": \"The following is a sample from a structured health dataset:\\n\\n{df}\\n\\nPlease provide 3-4 concise, data-driven insights focused on any observable patterns or anomalies.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# Apply chat template\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d531bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age group 0-17 years old has the highest number of diagnosis visits, with 18,420,000 visits. This is due to the fact that this age group is the most vulnerable to COVID-19, and the pandemic has had a significant impact on their health and well-being.\n",
      "\n",
      "2. The age group 18-44 years old has the second-highest number of diagnosis visits, with 13,253,000 visits. This is due to the fact that this age group is more likely to be employed and have access to healthcare services.\n",
      "\n",
      "3. The age group 55 years and above has the lowest number of diagnosis visits, with 1,297,000 visits. This is due to the fact that this age group is more likely to be elderly and have underlying health conditions that make them more vulnerable to COVID-19.\n",
      "\n",
      "By analyzing the structured health data, epidemiologists can gain insights into the COVID-19 pandemic's impact on different age groups and demographics. This information can be used to develop targeted interventions and policies to mitigate the effects of the pandemic on vulnerable populations.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and generate\n",
    "inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "result = decoded[len(chat_prompt):].strip()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d141ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pandas are the largest land mammals in the world, with adult males weighing up to 150 pounds and females weighing up to 100 pounds.\\n\\n2. Pandas are native to China and are found in the forests of Sichuan, Yunnan, and Gansu provinces.\\n\\n3. Pandas are primarily herbivores, feeding on bamboo shoots and leaves.\\n\\n4. Pandas are endangered due to habitat loss and poaching for their bones and meat.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Give three short facts about platypus (the animal).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817e7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"You are a data scientist. Analyze this metadata: age, region, income. Give 3 short points and stop generating.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e146e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda:0\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0825edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nYou are a helpful AI assistant.<|end|>\\n<|user|>\\nYou are a data scientist. Analyze this metadata: age, region, income. Give 3 short points and stop generating.<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9168e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449b7799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicCache' object has no attribute 'get_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m generate_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saksh\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saksh\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saksh\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\transformers\\generation\\utils.py:3550\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3546\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3549\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3550\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3552\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[32m   3553\u001b[39m     model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-mini-4k-instruct\\0a67737cc96d2554230f90338b163bc6380a2a85\\modeling_phi3.py:1292\u001b[39m, in \u001b[36mPhi3ForCausalLM.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001b[39m\n\u001b[32m   1290\u001b[39m     cache_length = past_key_values.get_seq_length()\n\u001b[32m   1291\u001b[39m     past_length = past_key_values.seen_tokens\n\u001b[32m-> \u001b[39m\u001b[32m1292\u001b[39m     max_cache_length = \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_max_length\u001b[49m()\n\u001b[32m   1293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1294\u001b[39m     cache_length = past_length = past_key_values[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape[\u001b[32m2\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicCache' object has no attribute 'get_max_length'"
     ]
    }
   ],
   "source": [
    "generate_ids = model.generate(**inputs,eos_token_id=tokenizer.eos_token_id, **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94114a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = \"\"\"\n",
    "\n",
    "GPT4 Correct System: You are a professional data scientist. GPT4 Correct User: The following is a sample from a structured health dataset:\n",
    "\n",
    "|    | Time Period    | Measure Type      | Measure    | Group   | Subgroup        | Estimate Type                 |     Estimate |   Standard Error |   Lower 95% CI |   Upper 95% CI | Reliable   |\n",
    "|---:|:---------------|:------------------|:-----------|:--------|:----------------|:------------------------------|-------------:|-----------------:|---------------:|---------------:|:-----------|\n",
    "|  0 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | Total   | All visits      | Visit count                   |   5.4013e+07 |        9.564e+06 |     3.5267e+07 |     7.2759e+07 | Yes        |\n",
    "|  1 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | Total   | All visits      | Visit rate (per 1,000 people) | 165.1        |       29.2       |   107.8        |   222.4        | Yes        |\n",
    "|  2 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 0-17 years old  | Visit count                   |   9.568e+06  |        1.869e+06 |     5.905e+06  |     1.3231e+07 | Yes        |\n",
    "|  3 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 0-17 years old  | Visit rate (per 1,000 people) | 130.3        |       25.5       |    80.4        |   180.2        | Yes        |\n",
    "|  4 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 18-44 years old | Visit count                   |   1.9253e+07 |        3.346e+06 |     1.2696e+07 |     2.5811e+07 | Yes        |\n",
    "\n",
    "Please provide 3-4 concise, data-driven insights focused on any observable patterns or anomalies. Suggestion: Provide more context and a clearer connection to the data. Provide more context and a clearer connection to the data. GPT4 Correct Assistant: 1. Age-based segmentation: The data shows a clear distinction in visit count and visit rate between different age groups. For example, the visit count for the 0-17 years old group is 9,568,000, while the visit count for the 18-44 years old group is 1,925,300. Similarly, the visit rate for the 0-17 years old group is 130.3, while the visit rate for the 18-44 years old group is 165.1. This suggests that younger age groups have a higher number of visits and visit rates, which could be attributed to factors such as higher prevalence of certain health conditions or more frequent healthcare utilization among younger individuals.\n",
    "\n",
    "2. Diagnosis Chapter: The data indicates that the total visit count for the entire dataset is 5,401,300 with a visit rate of 165.1. This suggests that the Diagnosis Chapter is a significant factor in determining the number of visits and visit rates. Further analysis could be conducted to identify specific diagnoses within the Diagnosis Chapter that contribute to these numbers.       \n",
    "\n",
    "3. Time Period: The data spans a six-month period (Jan.-Jun. 2022). While the dataset does not provide enough information to draw conclusions about seasonality or trends over time, it is worth noting that the data is relatively recent, which could be useful for understanding current healthcare trends and patterns.\n",
    "\n",
    "4. Reliable estimates: The dataset includes a \"Reliable\" column, which indicates whether the estimate is considered reliable or not. This could be useful for researchers and analysts to determine the credibility of the estimates and focus on more reliable data points for further analysis.\n",
    "================Evaluation=================\n",
    "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
    "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:24<00:00,  8.31s/it]\n",
    "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
    "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
    "==========Evaluation message===========\n",
    "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
    "==================decoded==================\n",
    "GPT4 Correct System: You are a very strict evaluator reviewing insights generated by a data analyst. Evaluate the insights based solely on the provided data, without assuming context. GPT4 Correct User: Here is the data:\n",
    "\n",
    "|    | Time Period    | Measure Type      | Measure    | Group   | Subgroup        | Estimate Type                 |     Estimate |   Standard Error |   Lower 95% CI |   Upper 95% CI | Reliable   |\n",
    "|---:|:---------------|:------------------|:-----------|:--------|:----------------|:------------------------------|-------------:|-----------------:|---------------:|---------------:|:-----------|\n",
    "|  0 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | Total   | All visits      | Visit count                   |   5.4013e+07 |        9.564e+06 |     3.5267e+07 |     7.2759e+07 | Yes        |\n",
    "|  1 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | Total   | All visits      | Visit rate (per 1,000 people) | 165.1        |       29.2       |   107.8        |   222.4        | Yes        |\n",
    "|  2 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 0-17 years old  | Visit count                   |   9.568e+06  |        1.869e+06 |     5.905e+06  |     1.3231e+07 | Yes        |\n",
    "|  3 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 0-17 years old  | Visit rate (per 1,000 people) | 130.3        |       25.5       |    80.4        |   180.2        | Yes        |\n",
    "|  4 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 18-44 years old | Visit count                   |   1.9253e+07 |        3.346e+06 |     1.2696e+07 |     2.5811e+07 | Yes        |\n",
    "\n",
    "Insight:\n",
    "\n",
    "GPT4 Correct System: You are a professional data scientist. GPT4 Correct User: The following is a sample from a structured health dataset:\n",
    "\n",
    "|    | Time Period    | Measure Type      | Measure    | Group   | Subgroup        | Estimate Type                 |     Estimate |   Standard Error |   Lower 95% CI |   Upper 95% CI | Reliable   |\n",
    "|---:|:---------------|:------------------|:-----------|:--------|:----------------|:------------------------------|-------------:|-----------------:|---------------:|---------------:|:-----------|\n",
    "|  0 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | Total   | All visits      | Visit count                   |   5.4013e+07 |        9.564e+06 |     3.5267e+07 |     7.2759e+07 | Yes        |\n",
    "|  1 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | Total   | All visits      | Visit rate (per 1,000 people) | 165.1        |       29.2       |   107.8        |   222.4        | Yes        |\n",
    "|  2 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 0-17 years old  | Visit count                   |   9.568e+06  |        1.869e+06 |     5.905e+06  |     1.3231e+07 | Yes        |\n",
    "|  3 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 0-17 years old  | Visit rate (per 1,000 people) | 130.3        |       25.5       |    80.4        |   180.2        | Yes        |\n",
    "|  4 | Jan.-Jun. 2022 | Diagnosis Chapter | All visits | By age  | 18-44 years old | Visit count                   |   1.9253e+07 |        3.346e+06 |     1.2696e+07 |     2.5811e+07 | Yes        |\n",
    "\n",
    "Please provide 3-4 concise, data-driven insights focused on any observable patterns or anomalies. Suggestion: Provide more context and a clearer connection to the data. Provide more context and a clearer connection to the data. GPT4 Correct Assistant: 1. Age-based segmentation: The data shows a clear distinction in visit count and visit rate between different age groups. For example, the visit count for the 0-17 years old group is 9,568,000, while the visit count for the 18-44 years old group is 1,925,300. Similarly, the visit rate for the 0-17 years old group is 130.3, while the visit rate for the 18-44 years old group is 165.1. This suggests that younger age groups have a higher number of visits and visit rates, which could be attributed to factors such as higher prevalence of certain health conditions or more frequent healthcare utilization among younger individuals.\n",
    "\n",
    "2. Diagnosis Chapter: The data indicates that the total visit count for the entire dataset is 5,401,300 with a visit rate of 165.1. This suggests that the Diagnosis Chapter is a significant factor in determining the number of visits and visit rates. Further analysis could be conducted to identify specific diagnoses within the Diagnosis Chapter that contribute to these numbers.       \n",
    "\n",
    "3. Time Period: The data spans a six-month period (Jan.-Jun. 2022). While the dataset does not provide enough information to draw conclusions about seasonality or trends over time, it is worth noting that the data is relatively recent, which could be useful for understanding current healthcare trends and patterns.\n",
    "\n",
    "4. Reliable estimates: The dataset includes a \"Reliable\" column, which indicates whether the estimate is considered reliable or not. This could be useful for researchers and analysts to determine the credibility of the estimates and focus on more reliable data points for further analysis.\n",
    "\n",
    "Rate the insight from 1 to 10.\n",
    "- 10 = flawless, highly novel, strongly data-supported.\n",
    "- 1 = wrong, vague, or irrelevant. You can give ratings between these two extremes.\n",
    "\n",
    "Then give a **very short** suggestion (max 10 words) to improve it.\n",
    "\n",
    "Format:\n",
    "Rating: <number>\n",
    "Suggestion: <suggestion> GPT4 Correct Assistant: Rating: 8\n",
    "Suggestion: Provide more context on the age groups and diagnoses within the Diagnosis Chapter, and explore potential reasons for the observed patterns.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f3ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = int(decode.split(\"\\nSuggestion: \")[1][-1])\n",
    "suggestion = prompt.split(\"\\nSuggestion: \")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a552725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provide more context on the age groups and diagnoses within the Diagnosis Chapter, and explore potential reasons for the observed patterns.\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6df591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provide more context on the age groups and diagnoses within the Diagnosis Chapter, and explore potential reasons for the observed patterns.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_metadata.prompts import"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
